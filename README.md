# Azure Sales ETL Pipeline

## Projekt-Ãœbersicht

Automatisierte ETL-Pipeline zur Verarbeitung von Sales- und Product-Daten in Azure Data Factory.

---

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚      â”‚   Data Flows     â”‚      â”‚    Staging      â”‚
â”‚   (Blob)        â”‚â”€â”€â”€â”€â”€>â”‚   (Transform)    â”‚â”€â”€â”€â”€â”€>â”‚   (Parquet)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â€¢ CSV/JSON              â€¢ Cleaning                 â€¢ Optimiert
  â€¢ Unstrukturiert        â€¢ Filtering                â€¢ Komprimiert
                          â€¢ Enrichment               â€¢ Partitioniert
```

---

## Datenquellen

### Input (Blob Storage: `raw/`)
| Datei | Format | Beschreibung |
|-------|--------|--------------|
| `sales_data.csv` | CSV | Verkaufstransaktionen |
| `products_api_response.json` | JSON | Produktkatalog (API Response) |
| `customers_dimension.csv` | CSV | Kundenstammdaten |

### Output (Blob Storage: `staging/`)
| Datei | Format | Beschreibung |
|-------|--------|--------------|
| `sales_cleaned.parquet` | Parquet | Bereinigte Sales-Daten |
| `products_active.parquet` | Parquet | Aktive Produkte (gefiltert) |

---

## Komponenten

### Linked Services
- **LS_BlobStorage_Sales**: Azure Blob Storage Verbindung

### Datasets
| Name | Typ | Pfad |
|------|-----|------|
| DS_Sales_CSV | DelimitedText | raw/sales_data.csv |
| DS_Products_JSON | JSON | raw/products_api_response.json |
| DS_Sales_Cleaned_Parquet | Parquet | staging/sales_cleaned.parquet |
| Parquet1 | Parquet | staging/products_active.parquet |

### Data Flows

#### 1. DF_Clean_Sales
```
Source (CSV)
    â†“
RemoveDuplicates (Aggregate)
    â†“
FilterNulls (Filter: !isNull(order_id))
    â†“
AddLoadDate (Derived Column: currentTimestamp())
    â†“
Sink (Parquet)
```

**Transformationen:**
- Duplikate entfernen (basierend auf order_id)
- Null-Werte in kritischen Feldern filtern
- Load-Timestamp hinzufÃ¼gen

#### 2. DF_Clean_Products
```
Source (JSON)
    â†“
FlattenData (Flatten: products array)
    â†“
FilterActive (Filter: status == 'active')
    â†“
Sink (Parquet)
```

**Transformationen:**
- JSON-Struktur flatten
- Nur aktive Produkte behalten
- Parquet-Optimierung

### Pipeline

**PL_Sales_ETL_Demo**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run Sales Cleaning       â”‚  (parallel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run Products Cleaning    â”‚  (parallel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Execution:** Beide Data Flows laufen parallel fÃ¼r maximale Performance

---

## Trigger

### 1. Schedule Trigger
- **Name:** TR_Daily_Sales_ETL
- **Frequenz:** TÃ¤glich um 02:00 Uhr
- **Zweck:** RegelmÃ¤ÃŸige Datenverarbeitung

### 2. Event Trigger (optional)
- **Name:** TR_OnNewFile_Sales
- **Event:** Neue Datei in `raw/` Ordner
- **Zweck:** Echtzeit-Verarbeitung bei Upload

---

## Monitoring

### Pipeline Runs
- **Location:** ADF Studio â†’ Monitor â†’ Pipeline runs
- **Metriken:**
  - Success Rate
  - Average Duration
  - Failed Runs

### Alerts
- **Email-Benachrichtigung** bei Pipeline-Fehler
- **Azure Monitor** Integration
- **Logic App** fÃ¼r erweiterte Notifications

---

## Deployment

### Voraussetzungen
- Azure Subscription
- Azure Data Factory Instance
- Azure Blob Storage Account
- Git Repository (GitHub)

### Setup-Schritte

1. **Repository klonen:**
   ```bash
   git clone https://github.com/[username]/azure-sales-etl-pipeline.git
   ```

2. **ADF mit Git verbinden:**
   - ADF Studio â†’ Manage â†’ Git configuration
   - Repository: azure-sales-etl-pipeline
   - Branch: main

3. **Linked Service konfigurieren:**
   - Update `LS_BlobStorage_Sales` mit deinem Storage Account
   - Connection String anpassen

4. **Testdaten hochladen:**
   ```bash
   az storage blob upload-batch      --account-name <storage-account>      --destination <container>/raw      --source ./sample-data
   ```

5. **Pipeline testen:**
   - Debug ausfÃ¼hren
   - Output prÃ¼fen
   - Staging-Dateien validieren

6. **Publish:**
   - Validate all
   - Publish all

---

## Testing

### Debug Mode
```bash
# In ADF Studio
1. Ã–ffne Pipeline: PL_Sales_ETL_Demo
2. Klicke "Debug"
3. Warte auf Completion (~3-5 Min)
4. PrÃ¼fe Output Tab
```

### Validierung
-  Beide Activities: Status = Succeeded
-  Parquet-Dateien in `staging/` vorhanden
-  Keine Fehler im Output Log

---

##  Performance

| Metrik | Wert |
|--------|------|
| Durchschnittliche Laufzeit | 3-5 Minuten |
| Parallelisierung | 2 Data Flows gleichzeitig |
| Compute Size | Small (4 vCores) |
| Datenvolumen | ~1-10 MB (Test), skalierbar |

---

##  Security

- **Managed Identity** fÃ¼r Storage-Zugriff
- **Key Vault** fÃ¼r Secrets (empfohlen)
- **RBAC** fÃ¼r ADF-Zugriff
- **Private Endpoints** (optional, fÃ¼r Prod)

---

##  Troubleshooting

### Problem: Pipeline schlÃ¤gt fehl
**LÃ¶sung:**
1. Monitor â†’ Pipeline runs â†’ Klick auf Failed run
2. PrÃ¼fe Error Message
3. Validiere Input-Daten
4. PrÃ¼fe Linked Service Connection

### Problem: Parquet-Dateien fehlen
**LÃ¶sung:**
1. PrÃ¼fe Sink-Konfiguration im Data Flow
2. Validiere Storage Account Permissions
3. PrÃ¼fe Container & Pfad

### Problem: Data Flow langsam
**LÃ¶sung:**
1. ErhÃ¶he Compute Size (Medium/Large)
2. Aktiviere Partitioning
3. Optimiere Transformationen

---

##  Ressourcen

- [Azure Data Factory Dokumentation](https://docs.microsoft.com/azure/data-factory/)
- [Data Flow Best Practices](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance)
- [Parquet Format Guide](https://parquet.apache.org/docs/)

---


## ğŸ“„ Lizenz

MIT License - siehe LICENSE Datei

