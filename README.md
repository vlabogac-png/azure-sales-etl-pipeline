# ğŸ­ Azure Sales ETL Pipeline

## ğŸ“‹ Projekt-Ãœbersicht

Automatisierte ETL-Pipeline zur Verarbeitung von Sales- und Product-Daten in Azure Data Factory.

---

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚      â”‚   Data Flows     â”‚      â”‚    Staging      â”‚
â”‚   (Blob)        â”‚â”€â”€â”€â”€â”€>â”‚   (Transform)    â”‚â”€â”€â”€â”€â”€>â”‚   (Parquet)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â€¢ CSV/JSON              â€¢ Cleaning                 â€¢ Optimiert
  â€¢ Unstrukturiert        â€¢ Filtering                â€¢ Komprimiert
                          â€¢ Enrichment               â€¢ Partitioniert
```

---

## ğŸ“ Datenquellen

### Input (Blob Storage: `raw/`)
| Datei | Format | Beschreibung |
|-------|--------|--------------|
| `sales_data.csv` | CSV | Verkaufstransaktionen |
| `products_api_response.json` | JSON | Produktkatalog (API Response) |
| `customers_dimension.csv` | CSV | Kundenstammdaten |

### Output (Blob Storage: `staging/`)
| Datei | Format | Beschreibung |
|-------|--------|--------------|
| `sales_cleaned.parquet` | Parquet | Bereinigte Sales-Daten |
| `products_active.parquet` | Parquet | Aktive Produkte (gefiltert) |

---

## ğŸ”§ Komponenten

### Linked Services
- **LS_BlobStorage_Sales**: Azure Blob Storage Verbindung

### Datasets
| Name | Typ | Pfad |
|------|-----|------|
| DS_Sales_CSV | DelimitedText | raw/sales_data.csv |
| DS_Products_JSON | JSON | raw/products_api_response.json |
| DS_Sales_Cleaned_Parquet | Parquet | staging/sales_cleaned.parquet |
| Parquet1 | Parquet | staging/products_active.parquet |

### Data Flows

#### 1. DF_Clean_Sales
```
Source (CSV)
    â†“
RemoveDuplicates (Aggregate)
    â†“
FilterNulls (Filter: !isNull(order_id))
    â†“
AddLoadDate (Derived Column: currentTimestamp())
    â†“
Sink (Parquet)
```

**Transformationen:**
- Duplikate entfernen (basierend auf order_id)
- Null-Werte in kritischen Feldern filtern
- Load-Timestamp hinzufÃ¼gen

#### 2. DF_Clean_Products
```
Source (JSON)
    â†“
FlattenData (Flatten: products array)
    â†“
FilterActive (Filter: status == 'active')
    â†“
Sink (Parquet)
```

**Transformationen:**
- JSON-Struktur flatten
- Nur aktive Produkte behalten
- Parquet-Optimierung

### Pipeline

**PL_Sales_ETL_Demo**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run Sales Cleaning       â”‚  (parallel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run Products Cleaning    â”‚  (parallel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Execution:** Beide Data Flows laufen parallel fÃ¼r maximale Performance

---

## âš™ï¸ Trigger

### 1. Schedule Trigger
- **Name:** TR_Daily_Sales_ETL
- **Frequenz:** TÃ¤glich um 02:00 Uhr
- **Zweck:** RegelmÃ¤ÃŸige Datenverarbeitung

### 2. Event Trigger (optional)
- **Name:** TR_OnNewFile_Sales
- **Event:** Neue Datei in `raw/` Ordner
- **Zweck:** Echtzeit-Verarbeitung bei Upload

---

## ğŸ“Š Monitoring

### Pipeline Runs
- **Location:** ADF Studio â†’ Monitor â†’ Pipeline runs
- **Metriken:**
  - Success Rate
  - Average Duration
  - Failed Runs

### Alerts
- **Email-Benachrichtigung** bei Pipeline-Fehler
- **Azure Monitor** Integration
- **Logic App** fÃ¼r erweiterte Notifications

---

## ğŸš€ Deployment

### Voraussetzungen
- Azure Subscription
- Azure Data Factory Instance
- Azure Blob Storage Account
- Git Repository (GitHub)

### Setup-Schritte

1. **Repository klonen:**
   ```bash
   git clone https://github.com/[username]/azure-sales-etl-pipeline.git
   ```

2. **ADF mit Git verbinden:**
   - ADF Studio â†’ Manage â†’ Git configuration
   - Repository: azure-sales-etl-pipeline
   - Branch: main

3. **Linked Service konfigurieren:**
   - Update `LS_BlobStorage_Sales` mit deinem Storage Account
   - Connection String anpassen

4. **Testdaten hochladen:**
   ```bash
   az storage blob upload-batch      --account-name <storage-account>      --destination <container>/raw      --source ./sample-data
   ```

5. **Pipeline testen:**
   - Debug ausfÃ¼hren
   - Output prÃ¼fen
   - Staging-Dateien validieren

6. **Publish:**
   - Validate all
   - Publish all

---

## ğŸ§ª Testing

### Debug Mode
```bash
# In ADF Studio
1. Ã–ffne Pipeline: PL_Sales_ETL_Demo
2. Klicke "Debug"
3. Warte auf Completion (~3-5 Min)
4. PrÃ¼fe Output Tab
```

### Validierung
- âœ… Beide Activities: Status = Succeeded
- âœ… Parquet-Dateien in `staging/` vorhanden
- âœ… Keine Fehler im Output Log

---

## ğŸ“ˆ Performance

| Metrik | Wert |
|--------|------|
| Durchschnittliche Laufzeit | 3-5 Minuten |
| Parallelisierung | 2 Data Flows gleichzeitig |
| Compute Size | Small (4 vCores) |
| Datenvolumen | ~1-10 MB (Test), skalierbar |

---

## ğŸ” Security

- **Managed Identity** fÃ¼r Storage-Zugriff
- **Key Vault** fÃ¼r Secrets (empfohlen)
- **RBAC** fÃ¼r ADF-Zugriff
- **Private Endpoints** (optional, fÃ¼r Prod)

---

## ğŸ› ï¸ Troubleshooting

### Problem: Pipeline schlÃ¤gt fehl
**LÃ¶sung:**
1. Monitor â†’ Pipeline runs â†’ Klick auf Failed run
2. PrÃ¼fe Error Message
3. Validiere Input-Daten
4. PrÃ¼fe Linked Service Connection

### Problem: Parquet-Dateien fehlen
**LÃ¶sung:**
1. PrÃ¼fe Sink-Konfiguration im Data Flow
2. Validiere Storage Account Permissions
3. PrÃ¼fe Container & Pfad

### Problem: Data Flow langsam
**LÃ¶sung:**
1. ErhÃ¶he Compute Size (Medium/Large)
2. Aktiviere Partitioning
3. Optimiere Transformationen

---

## ğŸ“š Ressourcen

- [Azure Data Factory Dokumentation](https://docs.microsoft.com/azure/data-factory/)
- [Data Flow Best Practices](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-performance)
- [Parquet Format Guide](https://parquet.apache.org/docs/)

---

## ğŸ‘¥ Team

- **Entwickler:** Vladislav Bogac
- **Projekt:** Azure Sales ETL Pipeline
- **Datum:** Januar 2026

---

## ğŸ“ Changelog

### v1.0.0 (2026-01-09)
- âœ… Initial Release
- âœ… Sales & Products Data Flows
- âœ… Pipeline mit Parallel Execution
- âœ… Schedule Trigger
- âœ… Monitoring & Alerts
- âœ… Git Integration

---

## ğŸ“„ Lizenz

MIT License - siehe LICENSE Datei

